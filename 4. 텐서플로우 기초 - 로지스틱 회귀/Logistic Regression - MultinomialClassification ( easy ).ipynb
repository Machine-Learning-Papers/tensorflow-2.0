{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression - MultinomialClassification ( easy )\n",
    "쉬운 데이터와 소프트맥스 함수를 사용한 간단한 로지스틱 회귀.\n",
    "> 출처\n",
    "1. Naver Edwith - https://www.edwith.org/boostcourse-dl-tensorflow/lecture/41854/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.random.set_seed(777)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 생성 및 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1. 2. 1. 1.]\n",
      " [2. 1. 3. 2.]\n",
      " [3. 1. 3. 4.]\n",
      " [4. 1. 5. 5.]\n",
      " [1. 7. 5. 5.]\n",
      " [1. 2. 5. 6.]\n",
      " [1. 6. 6. 6.]\n",
      " [1. 7. 7. 7.]], shape=(8, 4), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]], shape=(8, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 데이터 생성\n",
    "x_data = [[1, 2, 1, 1],\n",
    "          [2, 1, 3, 2],\n",
    "          [3, 1, 3, 4],\n",
    "          [4, 1, 5, 5],\n",
    "          [1, 7, 5, 5],\n",
    "          [1, 2, 5, 6],\n",
    "          [1, 6, 6, 6],\n",
    "          [1, 7, 7, 7]]\n",
    "y_data = [[0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 0, 1],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [0, 1, 0],\n",
    "          [1, 0, 0],\n",
    "          [1, 0, 0]]\n",
    "\n",
    "x_data = tf.constant(x_data, dtype = tf.float32)\n",
    "y_data = tf.constant(y_data, dtype = tf.float32)\n",
    "\n",
    "print(x_data)\n",
    "print(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(4, 3) dtype=float32, numpy=\n",
      "array([[-2.7697366e-01, -2.3162541e+00,  1.4861290e+00],\n",
      "       [-1.1312336e+00, -1.1059969e-01,  1.1424177e+00],\n",
      "       [-7.9997230e-01, -2.2132940e+00,  1.0443608e+00],\n",
      "       [ 1.5469295e+00, -6.0123701e-02,  6.6931121e-04]], dtype=float32)> <tf.Variable 'Variable:0' shape=(3,) dtype=float32, numpy=array([0.9083067, 1.7801584, 0.2447134], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "# 하이퍼 파라미터 설정\n",
    "num_classes = 3\n",
    "num_features = 4\n",
    "learning_rate = 0.1\n",
    "\n",
    "# 파라미터 설정\n",
    "W = tf.Variable(tf.random.normal([num_features, num_classes]))\n",
    "b = tf.Variable(tf.random.normal([num_classes]))\n",
    "\n",
    "print(W, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 함수 정의\n",
    "### 2-1. 비용함수 정의\n",
    "(1) 크로스 엔트로피를 수학적으로 정의하는 방법\n",
    "```python\n",
    "hypothsis = tf.nn.softmax(logits)\n",
    "cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.log(hypothesis), axis=1))\n",
    "```\n",
    "(2) 텐서플로우의 엔트로피 메서드 사용하는 방법\n",
    "```python\n",
    "logits = tf.matmul(X, W) + b\n",
    "cost_i = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=Y_one_hot)\n",
    "cost = tf.reduce_mean(cost_i)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "tf.Tensor(\n",
      "[[ 1.4636137  -0.72419554 -0.73941827]\n",
      " [ 1.6759303  -1.2196605  -0.45626992]\n",
      " [ 2.6729586  -1.835885   -0.8370737 ]\n",
      " [ 2.789841   -1.9541732  -0.83566797]], shape=(4, 3), dtype=float32)\n",
      "tf.Tensor([ 0.72310245 -0.35833293 -0.36476952], shape=(3,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# 가설함수\n",
    "def hypothesis(x):\n",
    "    return tf.nn.softmax(tf.matmul(x,W)+b)\n",
    "\n",
    "# 비용함수 ( Softmax )\n",
    "def cost_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(-tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1))\n",
    "\n",
    "def gradient_descent(x,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        cost = cost_function(y, hypothesis(x))\n",
    "        gradient = tape.gradient(cost, [W,b])\n",
    "    return gradient\n",
    "\n",
    "print(type(gradient_descent(x_data, y_data)))\n",
    "print(gradient_descent(x_data, y_data)[0]) # W의 미분값\n",
    "print(gradient_descent(x_data, y_data)[1]) # b의 미분값"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 cost: 0.36661726\n",
      "epoch: 100 cost: 0.33024806\n",
      "epoch: 200 cost: 0.29338238\n",
      "epoch: 300 cost: 0.25922564\n",
      "epoch: 400 cost: 0.24189408\n",
      "epoch: 500 cost: 0.2299619\n",
      "epoch: 600 cost: 0.21910591\n",
      "epoch: 700 cost: 0.20918411\n",
      "epoch: 800 cost: 0.20008187\n",
      "epoch: 900 cost: 0.19170329\n",
      "epoch: 1000 cost: 0.18396686\n",
      "epoch: 1100 cost: 0.17680332\n",
      "epoch: 1200 cost: 0.17015262\n",
      "epoch: 1300 cost: 0.16396323\n",
      "epoch: 1400 cost: 0.15818968\n",
      "epoch: 1500 cost: 0.1527927\n",
      "epoch: 1600 cost: 0.14773744\n",
      "epoch: 1700 cost: 0.14299339\n",
      "epoch: 1800 cost: 0.13853317\n",
      "epoch: 1900 cost: 0.13433293\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.optimizers.SGD(learning_rate)\n",
    "\n",
    "for i in range(2000):\n",
    "    grad = gradient_descent(x_data, y_data)\n",
    "    optimizer.apply_gradients(zip(grad, [W,b]))\n",
    "    \n",
    "    if(i % 100 == 0):\n",
    "        cost_value = cost_function(y_data, hypothesis(x_data)).numpy()\n",
    "        print(\"epoch:\",i,\"cost:\",cost_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7.8606006e-04 6.8775572e-02 9.3043834e-01]]\n",
      "tf.Tensor([2], shape=(1,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# 새로운 데이터를 통한 평가\n",
    "sample_data = [[2,1,3,2]] # answer_label = [[0, 0, 1]]\n",
    "sample_data = tf.constant(sample_data, dtype = tf.float32)\n",
    "a = hypothesis(sample_data)\n",
    "\n",
    "print(a.numpy())\n",
    "print(tf.argmax(a, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(tf.argmax(hypothesis(x_data), axis=1)) # 예측\n",
    "print(tf.argmax(y_data, axis=1)) # 정답\n",
    "\n",
    "correct = tf.cast(tf.equal(tf.argmax(hypothesis(x_data), axis=1) , tf.argmax(y_data, axis=1)), dtype=tf.float32)\n",
    "accuracy = tf.reduce_mean(correct)\n",
    "print(accuracy) # 정확도"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Class 화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class softmax_classifer(tf.keras.Model):\n",
    "    def __init__(self, nb_classes):\n",
    "        super(softmax_classifer, self).__init__()\n",
    "        self.W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
    "        self.b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
    "        \n",
    "    def softmax_regression(self, X):\n",
    "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
    "    \n",
    "    def cost_fn(self, X, Y):\n",
    "        logits = self.softmax_regression(X)\n",
    "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))        \n",
    "        return cost\n",
    "    \n",
    "    def grad_fn(self, X, Y):\n",
    "        with tf.GradientTape() as tape:\n",
    "            cost = self.cost_fn(x_data, y_data)\n",
    "            grads = tape.gradient(cost, self.variables)            \n",
    "            return grads\n",
    "    \n",
    "    def fit(self, X, Y, epochs=2000, verbose=500):\n",
    "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "        for i in range(epochs):\n",
    "            grads = self.grad_fn(X, Y)\n",
    "            optimizer.apply_gradients(zip(grads, self.variables))\n",
    "            if (i==0) | ((i+1)%verbose==0):\n",
    "                print('Loss at epoch %d: %f' %(i+1, self.cost_fn(X, Y).numpy()))\n",
    "            \n",
    "model = softmax_classifer(nb_classes)\n",
    "model.fit(x_data, y_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
