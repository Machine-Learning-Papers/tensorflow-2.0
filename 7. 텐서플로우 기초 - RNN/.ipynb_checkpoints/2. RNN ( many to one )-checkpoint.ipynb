{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN ( Many to One)\n",
    "> 참고\n",
    "1. https://github.com/deeplearningzerotoall/TensorFlow/blob/master/tf_2.x/lab-12-1-many-to-one-keras-eager.ipynb\n",
    "\n",
    "> 참고사항\n",
    "* map, lambda등등 python의 collection은 모두 'Python - Basic'노트북에 사용법을 정리해놨습니다.\n",
    "\n",
    "* RNN의 Many to One형태를 사용하여 Binary 분류를 하는 예제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Pre-Processing -1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['e', 'a', ' ', 'r', 't', 'w', 'o', 'd', 'g', 's', 'b']\n",
      "{'e': 0, 'a': 1, ' ': 2, 'r': 3, 't': 4, 'w': 5, 'o': 6, 'd': 7, 'g': 8, 's': 9, 'b': 10}\n",
      "{0: 'e', 1: 'a', 2: ' ', 3: 'r', 4: 't', 5: 'w', 6: 'o', 7: 'd', 8: 'g', 9: 's', 10: 'b'}\n"
     ]
    }
   ],
   "source": [
    "# Dataset\n",
    "data = ['good', 'bad', 'worse', 'so good','so bad','best']\n",
    "target = [1, 0, 0, 1, 0, 1]\n",
    "\n",
    "# Word Index ( Token )\n",
    "idx_char = list(set(\"\".join(data)))\n",
    "char_to_idx = {word:idx for idx, word in enumerate(idx_char)}\n",
    "idx_to_char = {idx:word for idx, word in enumerate(idx_char)}\n",
    "\n",
    "print(idx_char)\n",
    "print(char_to_idx)\n",
    "print(idx_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Pre-Processing - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8, 6, 6, 7], [10, 1, 7], [5, 6, 3, 9, 0], [9, 6, 2, 8, 6, 6, 7], [9, 6, 2, 10, 1, 7], [10, 0, 9, 4]]\n"
     ]
    }
   ],
   "source": [
    "# Token을 바탕으로 데이터 값 변환\n",
    "word_to_idx = lambda word: [char_to_idx.get(char) for char in word]\n",
    "data_x = list(map(word_to_idx, data)) # token을 바탕으로 데이터 변환.\n",
    "data_x_len = list(map(lambda word: len(word), data_x)) # 변환된 데이터의 길이\n",
    "\n",
    "print(data_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data & Pre-Processing - 3\n",
    "* Padding\n",
    "    * RNN으로 데이터를 훈련시킬때 Sequence길이가 일정해야 합니다. 데이터의 각각 서로 다른 길이를 맞추기 위해서 padding을 사용합니다. 기본적으로 0으로 패딩을 합니다.\n",
    "```python\n",
    "tf.keras.preprocessing.sequence.pad_sequence(\n",
    "    sequence = 데이터,\n",
    "    maxlen = Sequence의 최대 길이,\n",
    "    padding = 패딩을 어디로 줄지? ( 'post' )\n",
    "    truncating = 길이 줄이기 ( 'post' )\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---data_x---\n",
      "\n",
      "[[8, 6, 6, 7], [10, 1, 7], [5, 6, 3, 9, 0], [9, 6, 2, 8, 6, 6, 7], [9, 6, 2, 10, 1, 7], [10, 0, 9, 4]] \n",
      "\n",
      "---data_X---\n",
      "\n",
      "[[ 8  6  6  7  0  0  0  0  0  0]\n",
      " [10  1  7  0  0  0  0  0  0  0]\n",
      " [ 5  6  3  9  0  0  0  0  0  0]\n",
      " [ 9  6  2  8  6  6  7  0  0  0]\n",
      " [ 9  6  2 10  1  7  0  0  0  0]\n",
      " [10  0  9  4  0  0  0  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "max_sequence = 10\n",
    "data_X = tf.keras.preprocessing.sequence.pad_sequences(sequences = data_x, maxlen=max_sequence, padding='post', truncating='post')\n",
    "\n",
    "print(\"---data_x---\\n\")\n",
    "print(data_x,\"\\n\")\n",
    "\n",
    "print(\"---data_X---\\n\")\n",
    "print(data_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ((None, 10), (None,)), types: (tf.int32, tf.int32)>\n"
     ]
    }
   ],
   "source": [
    "batch_size = 2\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((data_X, target))\n",
    "dataset = dataset.shuffle(buffer_size = 4).batch(batch_size=2)\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Model\n",
    "\n",
    "### Hyper Parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding층에 들어가는 입출력 차원수\n",
    "input_dim = len(char_to_idx) # 13\n",
    "output_dim = len(char_to_idx)\n",
    "\n",
    "# RNN 훈련시 사용하는 파라미터\n",
    "hidden_size = 10 # num of units in each cell ( 셀마다의 unit 수 )\n",
    "num_classes = 2 # 분류 수 ( 1:positive , 0 : negative )\n",
    "learning_rate = 0.01\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding\n",
    "* Text는 비정형데이터이자, 컴퓨터가 이해할 수 없는 데이터이다. 그러므로 숫자화 시켜줄 필요가 있는데, 크게는 토큰 기반과 임베딩 방법이 있다.\n",
    "```python\n",
    "tf.keras.layers.Embedding(\n",
    "    input_dim = 입력 차원수,\n",
    "    output_dim = 출력 차원수,\n",
    "    mask_zero = 0값으로 패딩된 부분을 연산 제외 여부\n",
    "    ) \n",
    "```\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 10, 11)\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Embedding(input_dim=input_dim, output_dim=output_dim, trainable=False, mask_zero=True, input_length=max_sequence))\n",
    "print(model(data_X).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 10, 11)            121       \n",
      "_________________________________________________________________\n",
      "rnn (RNN)                    (None, 10)                220       \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 2)                 22        \n",
      "=================================================================\n",
      "Total params: 363\n",
      "Trainable params: 242\n",
      "Non-trainable params: 121\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# RNN Cell\n",
    "cell = tf.keras.layers.SimpleRNNCell(units=hidden_size, activation='tanh')\n",
    "model.add(tf.keras.layers.RNN(cell=cell))\n",
    "model.add(tf.keras.layers.Dense(units=num_classes))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cost\n",
    "def cost_function(y_true, y_pred):\n",
    "    return tf.reduce_mean(tf.keras.losses.sparse_categorical_crossentropy(y_true=y_true, y_pred=y_pred))\n",
    "\n",
    "optimizer = tf.optimizers.Adam(learning_rate = learning_rate)\n",
    "\n",
    "def run_optimizer(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predict = model(x)\n",
    "        cost = cost_function(y, predict)\n",
    "    grads = tape.gradient(cost, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 5, cost : 0.57762\n",
      "epoch : 10, cost : 0.57762\n",
      "epoch : 15, cost : 0.46210\n",
      "epoch : 20, cost : 0.46210\n",
      "epoch : 25, cost : 0.46210\n",
      "epoch : 30, cost : 0.46210\n",
      "tf.Tensor(1.0, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "cost_log = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    cost_avg = 0\n",
    "    training_step = 0\n",
    "\n",
    "    for x_batch, y_batch in dataset:\n",
    "        cost = run_optimizer(model, x_batch, y_batch)\n",
    "        cost_avg+=cost\n",
    "        training_step+=1\n",
    "\n",
    "    cost_avg /= training_step\n",
    "    cost_log.append(cost_avg)\n",
    "\n",
    "    if (epoch+1) % 5 == 0:\n",
    "        print(\"epoch : {}, cost : {:.5f}\".format(epoch+1, cost_avg))\n",
    "\n",
    "hypothesis = tf.argmax(model(data_X), axis=-1)\n",
    "print(tf.reduce_mean(tf.cast(tf.equal(hypothesis, target),dtype=tf.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
